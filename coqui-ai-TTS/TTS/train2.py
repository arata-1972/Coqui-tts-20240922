import os

from trainer import Trainer, TrainerArgs

from TTS.tts.configs.shared_configs import BaseDatasetConfig
from TTS.tts.configs.vits_config import VitsConfig
from TTS.tts.datasets import load_tts_samples
from TTS.tts.models.vits import Vits, VitsAudioConfig
from TTS.tts.utils.text.tokenizer import TTSTokenizer
from TTS.utils.audio import AudioProcessor

output_path = os.path.dirname(os.path.abspath(__file__))
dataset_config = BaseDatasetConfig(
    formatter="ljspeech", meta_file_train="metadata.csv", 
    path=os.path.join(output_path, "../recipes/ljspeech/LJSpeech-1.1/")
)
audio_config = VitsAudioConfig(
    sample_rate=22050, 
    win_length=1024, 
    hop_length=256, 
    num_mels=80,
    #mel_fmin=0,
    #mel_fmax=None
    mel_fmin=50,  # ä¸‹é™å‘¨æ³¢æ•°ã‚’è¨­å®š
    mel_fmax=8000  # ä¸Šé™å‘¨æ³¢æ•°ã‚’è¨­å®š
)

config = VitsConfig(
    audio=audio_config,
    run_name="vits_ljspeech",
    #batch_size=8,
    batch_size=4,
    #eval_batch_size=4,
    eval_batch_size=2,
    batch_group_size=5,q
    num_loader_workers=8,
    num_eval_loader_workers=4,
    run_eval=True,
    test_delay_epochs=-1,
    epochs=1000,
    text_cleaner="phoneme_cleaners",
    use_phonemes=True,
    phoneme_language="nan-tw",
    phoneme_cache_path=os.path.join(output_path, "phoneme_cache"),
    compute_input_seq_cache=True,
    print_step=25,
    print_eval=True,
    mixed_precision=True,
    #mixed_precision=False,  # æ··åˆç²¾åº¦ã‚’ç„¡åŠ¹åŒ–
    output_path=output_path,
    datasets=[dataset_config],
    cudnn_benchmark=False,
    #eval_split_size=100,
    eval_split_size = 0.15

)

# ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ—ãƒ­ã‚»ãƒƒã‚µã®åˆæœŸåŒ–
ap = AudioProcessor.init_from_config(config)

# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®åˆæœŸåŒ–
tokenizer, config = TTSTokenizer.init_from_config(config)

print(dir(tokenizer))



# ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªã®èª­ã¿è¾¼ã¿æ™‚ã«ãƒ¢ãƒãƒ©ãƒ«ã«å¤‰æ›
def load_audio_mono(audio_path, ap):
    wav = ap.load_wav(audio_path)
    if wav.ndim > 1:  # ã‚¹ãƒ†ãƒ¬ã‚ªã®å ´åˆã€ãƒ¢ãƒãƒ©ãƒ«ã«å¤‰æ›
        wav = wav.mean(axis=1)
    return wav


# ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã§ãƒ†ãƒ³ã‚½ãƒ«ã®æ¬¡å…ƒã‚’åˆã‚ã›ã‚‹éƒ¨åˆ†ã®ä¿®æ­£
def collate_fn(batch):
    max_len = max([wav.shape[-1] for wav in batch])  # éŸ³å£°ã®æœ€å¤§é•·ã‚’å–å¾—
    wav_padded = torch.zeros((len(batch), 1, max_len))  # 1ãƒãƒ£ãƒãƒ«ã«å¼·åˆ¶
    for i, wav in enumerate(batch):
        if wav.ndim == 1:  # 1ãƒãƒ£ãƒãƒ«ï¼ˆãƒ¢ãƒãƒ©ãƒ«ï¼‰ã®å ´åˆ
            wav = wav.unsqueeze(0)  # æ¬¡å…ƒã‚’è¿½åŠ ã—ã¦3æ¬¡å…ƒã«
        wav_padded[i, :, :wav.shape[1]] = torch.FloatTensor(wav)
    return wav_padded


# ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã‹ã‚’ç¢ºèªã—ã€æ¬ æãƒ•ã‚¡ã‚¤ãƒ«ã‚’é™¤å¤–ã™ã‚‹é–¢æ•°
def filter_and_load_audio_files(samples, ap):
    valid_samples = []
    for sample in samples:
        audio_path = sample['audio_file']
        if os.path.exists(audio_path):
            wav = load_audio_mono(audio_path, ap)  # ãƒ¢ãƒãƒ©ãƒ«ã«å¤‰æ›
            sample['audio'] = wav  # ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ‡ãƒ¼ã‚¿ã‚’ã‚µãƒ³ãƒ—ãƒ«ã«è¿½åŠ 
            valid_samples.append(sample)
        else:
            print(f"File not found, skipping: {audio_path}")
    return valid_samples


# ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
train_samples, eval_samples = load_tts_samples(
    dataset_config,
    eval_split=True,
    eval_split_max_size=config.eval_split_max_size,
    eval_split_size=config.eval_split_size,
)

# AudioProcessorã‚’ä½¿ã£ã¦éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
ap = AudioProcessor.init_from_config(config)

# ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã¨æ¬ æãƒ•ã‚¡ã‚¤ãƒ«ã®é™¤å¤–
train_samples = filter_and_load_audio_files(train_samples, ap)
eval_samples = filter_and_load_audio_files(eval_samples, ap)



# metadata.csvã®å†…å®¹ã‚’è¡¨ç¤º
with open(os.path.join(dataset_config.path, dataset_config.meta_file_train), 'r', encoding='utf-8') as f:
    for i, line in enumerate(f):
        if i < 10:  # æœ€åˆã®10è¡Œã‚’è¡¨ç¤º
            print(line)

            

# ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚µãƒ³ãƒ—ãƒ«ã®æœ€åˆã®æ•°ä»¶ã‚’è¡¨ç¤º
print(f"Train Samples: {train_samples[:5]}")
print(f"Eval Samples: {eval_samples[:5]}")

# ã‚µãƒ³ãƒ—ãƒ«ãŒé©åˆ‡ã‹ã©ã†ã‹ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
#train_samples = [s for s in train_samples if len(s['text']) > 1]
#eval_samples = [s for s in eval_samples if len(s['text']) > 1]

print(f"ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(train_samples)}")
print(f"è©•ä¾¡ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(eval_samples)}")

# init model
model = Vits(config, ap, tokenizer, speaker_manager=None)

# init the trainer and ğŸš€
trainer = Trainer(
    TrainerArgs(),
    config,
    output_path,
    model=model,
    train_samples=train_samples,
    eval_samples=eval_samples,
)


trainer.fit()
